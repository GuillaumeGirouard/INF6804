{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchyolo import YoloHub\n",
    "from ocsort.ocsort import OCSort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2023-3-31 Python-3.8.16 torch-1.12.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 270 layers, 7235389 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m YoloHub(\n\u001b[0;32m      2\u001b[0m     config_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./default_config.yaml\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     model_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myolov5\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     model_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myolov5s.pt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(source\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./frames/frame0.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchyolo\\predict.py:46\u001b[0m, in \u001b[0;36mYoloHub.predict\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, source: \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 46\u001b[0m     prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(source\u001b[39m=\u001b[39;49msource)\n\u001b[0;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m prediction\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchyolo\\modelhub\\yolov5.py:43\u001b[0m, in \u001b[0;36mYolov5DetectionModel.predict\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m     41\u001b[0m video_writer \u001b[39m=\u001b[39m create_video_writer(video_path\u001b[39m=\u001b[39msource, output_path\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_path)\n\u001b[0;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m img_src, img_path, vid_cap \u001b[39min\u001b[39;00m tqdm(dataset):\n\u001b[1;32m---> 43\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(img_src, size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_size)\n\u001b[0;32m     44\u001b[0m     \u001b[39mfor\u001b[39;00m image_id, prediction \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(results\u001b[39m.\u001b[39mpred):\n\u001b[0;32m     45\u001b[0m         \u001b[39mfor\u001b[39;00m pred \u001b[39min\u001b[39;00m prediction\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy():\n",
      "File \u001b[1;32mc:\\Users\\ggiro\\.conda\\envs\\TP3_INF6804\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m   1131\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n\u001b[0;32m   1132\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ggiro\\.conda\\envs\\TP3_INF6804\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\yolov5\\models\\common.py:716\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[1;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[39m# Post-process\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39mwith\u001b[39;00m dt[\u001b[39m2\u001b[39m]:\n\u001b[1;32m--> 716\u001b[0m     y \u001b[39m=\u001b[39m non_max_suppression(y \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdmb \u001b[39melse\u001b[39;49;00m y[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m    717\u001b[0m                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconf,\n\u001b[0;32m    718\u001b[0m                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miou,\n\u001b[0;32m    719\u001b[0m                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses,\n\u001b[0;32m    720\u001b[0m                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magnostic,\n\u001b[0;32m    721\u001b[0m                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmulti_label,\n\u001b[0;32m    722\u001b[0m                             max_det\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_det)  \u001b[39m# NMS\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[0;32m    724\u001b[0m         scale_boxes(shape1, y[i][:, :\u001b[39m4\u001b[39m], shape0[i])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\yolov5\\utils\\general.py:977\u001b[0m, in \u001b[0;36mnon_max_suppression\u001b[1;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nm)\u001b[0m\n\u001b[0;32m    975\u001b[0m c \u001b[39m=\u001b[39m x[:, \u001b[39m5\u001b[39m:\u001b[39m6\u001b[39m] \u001b[39m*\u001b[39m (\u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m agnostic \u001b[39melse\u001b[39;00m max_wh)  \u001b[39m# classes\u001b[39;00m\n\u001b[0;32m    976\u001b[0m boxes, scores \u001b[39m=\u001b[39m x[:, :\u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m c, x[:, \u001b[39m4\u001b[39m]  \u001b[39m# boxes (offset by class), scores\u001b[39;00m\n\u001b[1;32m--> 977\u001b[0m i \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mnms(boxes, scores, iou_thres)  \u001b[39m# NMS\u001b[39;00m\n\u001b[0;32m    978\u001b[0m i \u001b[39m=\u001b[39m i[:max_det]  \u001b[39m# limit detections\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[39mif\u001b[39;00m merge \u001b[39mand\u001b[39;00m (\u001b[39m1\u001b[39m \u001b[39m<\u001b[39m n \u001b[39m<\u001b[39m \u001b[39m3E3\u001b[39m):  \u001b[39m# Merge NMS (boxes merged using weighted mean)\u001b[39;00m\n\u001b[0;32m    980\u001b[0m     \u001b[39m# update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ggiro\\.conda\\envs\\TP3_INF6804\\lib\\site-packages\\torchvision\\ops\\boxes.py:39\u001b[0m, in \u001b[0;36mnms\u001b[1;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m     38\u001b[0m     _log_api_usage_once(nms)\n\u001b[1;32m---> 39\u001b[0m _assert_has_ops()\n\u001b[0;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mtorchvision\u001b[39m.\u001b[39mnms(boxes, scores, iou_threshold)\n",
      "File \u001b[1;32mc:\\Users\\ggiro\\.conda\\envs\\TP3_INF6804\\lib\\site-packages\\torchvision\\extension.py:33\u001b[0m, in \u001b[0;36m_assert_has_ops\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_has_ops\u001b[39m():\n\u001b[0;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_ops():\n\u001b[1;32m---> 33\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load custom C++ ops. This can happen if your PyTorch and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtorchvision versions are incompatible, or if you had errors while compiling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtorchvision from source. For further information on the compatible versions, check \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/pytorch/vision#installation for the compatibility matrix. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease check your PyTorch version with torch.__version__ and your torchvision \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mversion with torchvision.__version__ and verify if they are compatible, and if not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mplease reinstall torchvision so that it matches your PyTorch install.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install."
     ]
    }
   ],
   "source": [
    "model = YoloHub(\n",
    "    config_path=\"./default_config.yaml\",\n",
    "    model_type=\"yolov5\",\n",
    "    model_path=\"yolov5s.pt\",\n",
    ")\n",
    "\n",
    "result = model.predict(source=\"./frames/frame0.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tracker \u001b[39m=\u001b[39m OCSort()\n\u001b[1;32m----> 2\u001b[0m dets \u001b[39m=\u001b[39m yl\u001b[39m.\u001b[39mdetector(image)\n\u001b[0;32m      3\u001b[0m online_targets \u001b[39m=\u001b[39m tracker\u001b[39m.\u001b[39mupdate(dets)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yl' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tracker = OCSort()\n",
    "#dets = yl.detector(image)\n",
    "online_targets = tracker.update(dets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
